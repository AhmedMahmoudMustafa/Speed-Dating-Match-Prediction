{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c6fff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb11f1",
   "metadata": {},
   "source": [
    "## Loading the Training and Testing  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a394da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "0       0    3       2    14     18         2       2.0     14       12   \n",
       "1       1   14       1     3     10         2       NaN      8        8   \n",
       "2       1   14       1    13     10         8       8.0     10       10   \n",
       "3       1   38       2     9     20        18      13.0      6        7   \n",
       "4       1   24       2    14     20         6       6.0     20       17   \n",
       "\n",
       "     pid  ...  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
       "0  372.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "1   63.0  ...      8.0       8.0     7.0     8.0      NaN      NaN       NaN   \n",
       "2  331.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "3  200.0  ...      9.0       8.0     8.0     6.0      NaN      NaN       NaN   \n",
       "4  357.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "\n",
       "   fun5_3  amb5_3    id  \n",
       "0     NaN     NaN  2583  \n",
       "1     NaN     NaN  6830  \n",
       "2     NaN     NaN  4840  \n",
       "3     NaN     NaN  5508  \n",
       "4     NaN     NaN  4828  \n",
       "\n",
       "[5 rows x 192 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the first 5 rows of the training data \n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb176000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "0       0    5       2     2     16         3       NaN     13       13   \n",
       "1       0   33       2    14     18         6       6.0      4        8   \n",
       "2       1    6       2     9     20        10      16.0     15       19   \n",
       "3       1   26       2     2     19        15       NaN      8       10   \n",
       "4       0   29       2     7     16         7       7.0     10        5   \n",
       "\n",
       "     pid  ...  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
       "0   52.0  ...      7.0       8.0     6.0     8.0      NaN      NaN       NaN   \n",
       "1  368.0  ...      8.0       7.0     7.0     8.0      6.0      7.0       6.0   \n",
       "2  212.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "3   30.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "4  162.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "\n",
       "   fun5_3  amb5_3    id  \n",
       "0     NaN     NaN   934  \n",
       "1     5.0     5.0  6539  \n",
       "2     NaN     NaN  6757  \n",
       "3     NaN     NaN  2275  \n",
       "4     NaN     NaN  1052  \n",
       "\n",
       "[5 rows x 191 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the first 5 rows of the test data \n",
    "df1 = pd.read_csv('test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5fd53e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5909, 192), (2469, 191))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension of the data \n",
    "df.shape, df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cf13203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the dublicates \n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6b18bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'idg',\n",
       " 'condtn',\n",
       " 'wave',\n",
       " 'round',\n",
       " 'position',\n",
       " 'positin1',\n",
       " 'order',\n",
       " 'partner',\n",
       " 'pid',\n",
       " 'match',\n",
       " 'int_corr',\n",
       " 'samerace',\n",
       " 'age_o',\n",
       " 'race_o',\n",
       " 'pf_o_att',\n",
       " 'pf_o_sin',\n",
       " 'pf_o_int',\n",
       " 'pf_o_fun',\n",
       " 'pf_o_amb',\n",
       " 'pf_o_sha',\n",
       " 'attr_o',\n",
       " 'sinc_o',\n",
       " 'intel_o',\n",
       " 'fun_o',\n",
       " 'amb_o',\n",
       " 'shar_o',\n",
       " 'like_o',\n",
       " 'prob_o',\n",
       " 'met_o',\n",
       " 'age',\n",
       " 'field',\n",
       " 'field_cd',\n",
       " 'undergra',\n",
       " 'mn_sat',\n",
       " 'tuition',\n",
       " 'race',\n",
       " 'imprace',\n",
       " 'imprelig',\n",
       " 'from',\n",
       " 'zipcode',\n",
       " 'income',\n",
       " 'goal',\n",
       " 'date',\n",
       " 'go_out',\n",
       " 'career',\n",
       " 'career_c',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'exphappy',\n",
       " 'expnum',\n",
       " 'attr1_1',\n",
       " 'sinc1_1',\n",
       " 'intel1_1',\n",
       " 'fun1_1',\n",
       " 'amb1_1',\n",
       " 'shar1_1',\n",
       " 'attr4_1',\n",
       " 'sinc4_1',\n",
       " 'intel4_1',\n",
       " 'fun4_1',\n",
       " 'amb4_1',\n",
       " 'shar4_1',\n",
       " 'attr2_1',\n",
       " 'sinc2_1',\n",
       " 'intel2_1',\n",
       " 'fun2_1',\n",
       " 'amb2_1',\n",
       " 'shar2_1',\n",
       " 'attr3_1',\n",
       " 'sinc3_1',\n",
       " 'fun3_1',\n",
       " 'intel3_1',\n",
       " 'amb3_1',\n",
       " 'attr5_1',\n",
       " 'sinc5_1',\n",
       " 'intel5_1',\n",
       " 'fun5_1',\n",
       " 'amb5_1',\n",
       " 'attr',\n",
       " 'sinc',\n",
       " 'intel',\n",
       " 'fun',\n",
       " 'amb',\n",
       " 'shar',\n",
       " 'like',\n",
       " 'prob',\n",
       " 'met',\n",
       " 'match_es',\n",
       " 'attr1_s',\n",
       " 'sinc1_s',\n",
       " 'intel1_s',\n",
       " 'fun1_s',\n",
       " 'amb1_s',\n",
       " 'shar1_s',\n",
       " 'attr3_s',\n",
       " 'sinc3_s',\n",
       " 'intel3_s',\n",
       " 'fun3_s',\n",
       " 'amb3_s',\n",
       " 'satis_2',\n",
       " 'length',\n",
       " 'numdat_2',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'attr1_2',\n",
       " 'sinc1_2',\n",
       " 'intel1_2',\n",
       " 'fun1_2',\n",
       " 'amb1_2',\n",
       " 'shar1_2',\n",
       " 'attr4_2',\n",
       " 'sinc4_2',\n",
       " 'intel4_2',\n",
       " 'fun4_2',\n",
       " 'amb4_2',\n",
       " 'shar4_2',\n",
       " 'attr2_2',\n",
       " 'sinc2_2',\n",
       " 'intel2_2',\n",
       " 'fun2_2',\n",
       " 'amb2_2',\n",
       " 'shar2_2',\n",
       " 'attr3_2',\n",
       " 'sinc3_2',\n",
       " 'intel3_2',\n",
       " 'fun3_2',\n",
       " 'amb3_2',\n",
       " 'attr5_2',\n",
       " 'sinc5_2',\n",
       " 'intel5_2',\n",
       " 'fun5_2',\n",
       " 'amb5_2',\n",
       " 'you_call',\n",
       " 'them_cal',\n",
       " 'date_3',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr1_3',\n",
       " 'sinc1_3',\n",
       " 'intel1_3',\n",
       " 'fun1_3',\n",
       " 'amb1_3',\n",
       " 'shar1_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr3_3',\n",
       " 'sinc3_3',\n",
       " 'intel3_3',\n",
       " 'fun3_3',\n",
       " 'amb3_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3',\n",
       " 'id']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the feature\n",
    "[x for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20b2343e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>-0.004047</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>-0.056275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169387</td>\n",
       "      <td>0.011476</td>\n",
       "      <td>-0.153701</td>\n",
       "      <td>-0.066626</td>\n",
       "      <td>-0.133302</td>\n",
       "      <td>-0.277085</td>\n",
       "      <td>0.080227</td>\n",
       "      <td>-0.065562</td>\n",
       "      <td>0.069091</td>\n",
       "      <td>0.006340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idg</th>\n",
       "      <td>0.032132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330587</td>\n",
       "      <td>0.093823</td>\n",
       "      <td>0.391918</td>\n",
       "      <td>0.164705</td>\n",
       "      <td>0.174651</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.139034</td>\n",
       "      <td>0.088372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050350</td>\n",
       "      <td>-0.060674</td>\n",
       "      <td>-0.041080</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>-0.018915</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>-0.145645</td>\n",
       "      <td>-0.036693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condtn</th>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.330587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219735</td>\n",
       "      <td>0.820898</td>\n",
       "      <td>0.331013</td>\n",
       "      <td>0.306722</td>\n",
       "      <td>0.331402</td>\n",
       "      <td>0.322467</td>\n",
       "      <td>0.218009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.061796</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>-0.087502</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.106486</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>-0.044675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wave</th>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.093823</td>\n",
       "      <td>0.219735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228917</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.061166</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.087904</td>\n",
       "      <td>0.996714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>-0.104562</td>\n",
       "      <td>0.053971</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.092556</td>\n",
       "      <td>-0.001751</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.047841</td>\n",
       "      <td>-0.001863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.391918</td>\n",
       "      <td>0.820898</td>\n",
       "      <td>0.228917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380632</td>\n",
       "      <td>0.368352</td>\n",
       "      <td>0.397952</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.218901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>0.104829</td>\n",
       "      <td>0.092951</td>\n",
       "      <td>-0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinc5_3</th>\n",
       "      <td>-0.277085</td>\n",
       "      <td>-0.018915</td>\n",
       "      <td>-0.087502</td>\n",
       "      <td>-0.001751</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>0.111991</td>\n",
       "      <td>0.125594</td>\n",
       "      <td>-0.035897</td>\n",
       "      <td>-0.002601</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.315857</td>\n",
       "      <td>0.180240</td>\n",
       "      <td>0.122856</td>\n",
       "      <td>0.190198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498313</td>\n",
       "      <td>0.335823</td>\n",
       "      <td>0.261307</td>\n",
       "      <td>0.051102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intel5_3</th>\n",
       "      <td>0.080227</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>0.089283</td>\n",
       "      <td>0.106791</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288062</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.240322</td>\n",
       "      <td>0.268646</td>\n",
       "      <td>0.355876</td>\n",
       "      <td>0.498313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262987</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.003512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun5_3</th>\n",
       "      <td>-0.065562</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>0.106486</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.104829</td>\n",
       "      <td>0.135484</td>\n",
       "      <td>0.232412</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.096788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160204</td>\n",
       "      <td>0.295997</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.331533</td>\n",
       "      <td>0.412403</td>\n",
       "      <td>0.335823</td>\n",
       "      <td>0.262987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>0.003349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amb5_3</th>\n",
       "      <td>0.069091</td>\n",
       "      <td>-0.145645</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.047841</td>\n",
       "      <td>0.092951</td>\n",
       "      <td>0.058825</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.043115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135499</td>\n",
       "      <td>0.353093</td>\n",
       "      <td>0.429332</td>\n",
       "      <td>0.619087</td>\n",
       "      <td>0.194078</td>\n",
       "      <td>0.261307</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.006340</td>\n",
       "      <td>-0.036693</td>\n",
       "      <td>-0.044675</td>\n",
       "      <td>-0.001863</td>\n",
       "      <td>-0.029700</td>\n",
       "      <td>-0.026828</td>\n",
       "      <td>-0.011816</td>\n",
       "      <td>-0.016321</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020346</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.003579</td>\n",
       "      <td>-0.017013</td>\n",
       "      <td>0.051102</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>-0.007695</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender       idg    condtn      wave     round  position  \\\n",
       "gender    1.000000  0.032132 -0.000875 -0.004192  0.017755 -0.004047   \n",
       "idg       0.032132  1.000000  0.330587  0.093823  0.391918  0.164705   \n",
       "condtn   -0.000875  0.330587  1.000000  0.219735  0.820898  0.331013   \n",
       "wave     -0.004192  0.093823  0.219735  1.000000  0.228917  0.079820   \n",
       "round     0.017755  0.391918  0.820898  0.228917  1.000000  0.380632   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "sinc5_3  -0.277085 -0.018915 -0.087502 -0.001751 -0.035242  0.111991   \n",
       "intel5_3  0.080227 -0.093206  0.051449  0.014611 -0.012896  0.089283   \n",
       "fun5_3   -0.065562 -0.061079  0.106486  0.088716  0.104829  0.135484   \n",
       "amb5_3    0.069091 -0.145645  0.123314  0.047841  0.092951  0.058825   \n",
       "id        0.006340 -0.036693 -0.044675 -0.001863 -0.029700 -0.026828   \n",
       "\n",
       "          positin1     order   partner       pid  ...   sinc3_3  intel3_3  \\\n",
       "gender    0.000410  0.009850  0.010318 -0.056275  ... -0.169387  0.011476   \n",
       "idg       0.174651  0.161976  0.139034  0.088372  ... -0.050350 -0.060674   \n",
       "condtn    0.306722  0.331402  0.322467  0.218009  ...  0.044158  0.061796   \n",
       "wave      0.061166  0.093478  0.087904  0.996714  ...  0.036948 -0.104562   \n",
       "round     0.368352  0.397952  0.390320  0.218901  ...  0.036600  0.022696   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "sinc5_3   0.125594 -0.035897 -0.002601  0.021372  ...  0.608213  0.315857   \n",
       "intel5_3  0.106791 -0.004632  0.003345  0.009585  ...  0.288062  0.683871   \n",
       "fun5_3    0.232412  0.016419  0.031795  0.096788  ...  0.160204  0.295997   \n",
       "amb5_3    0.030543  0.029174  0.050511  0.043115  ...  0.135499  0.353093   \n",
       "id       -0.011816 -0.016321  0.006595 -0.002027  ...  0.020346  0.004193   \n",
       "\n",
       "            fun3_3    amb3_3   attr5_3   sinc5_3  intel5_3    fun5_3  \\\n",
       "gender   -0.153701 -0.066626 -0.133302 -0.277085  0.080227 -0.065562   \n",
       "idg      -0.041080 -0.005502  0.001764 -0.018915 -0.093206 -0.061079   \n",
       "condtn    0.069298  0.027447  0.077571 -0.087502  0.051449  0.106486   \n",
       "wave      0.053971  0.000520  0.092556 -0.001751  0.014611  0.088716   \n",
       "round     0.059530  0.013107  0.034647 -0.035242 -0.012896  0.104829   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "sinc5_3   0.180240  0.122856  0.190198  1.000000  0.498313  0.335823   \n",
       "intel5_3  0.240322  0.268646  0.355876  0.498313  1.000000  0.262987   \n",
       "fun5_3    0.771447  0.331533  0.412403  0.335823  0.262987  1.000000   \n",
       "amb5_3    0.429332  0.619087  0.194078  0.261307  0.422743  0.379768   \n",
       "id        0.000966 -0.003579 -0.017013  0.051102  0.003512  0.003349   \n",
       "\n",
       "            amb5_3        id  \n",
       "gender    0.069091  0.006340  \n",
       "idg      -0.145645 -0.036693  \n",
       "condtn    0.123314 -0.044675  \n",
       "wave      0.047841 -0.001863  \n",
       "round     0.092951 -0.029700  \n",
       "...            ...       ...  \n",
       "sinc5_3   0.261307  0.051102  \n",
       "intel5_3  0.422743  0.003512  \n",
       "fun5_3    0.379768  0.003349  \n",
       "amb5_3    1.000000 -0.007695  \n",
       "id       -0.007695  1.000000  \n",
       "\n",
       "[184 rows x 184 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b5b03ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5909 entries, 0 to 5908\n",
      "Columns: 192 entries, gender to id\n",
      "dtypes: float64(173), int64(11), object(8)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#info about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9687027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_in_3    5449\n",
       "numdat_3    4849\n",
       "expnum      4627\n",
       "amb7_2      4519\n",
       "sinc7_2     4519\n",
       "            ... \n",
       "position       0\n",
       "round          0\n",
       "wave           0\n",
       "condtn         0\n",
       "id             0\n",
       "Length: 192, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum the nulls and sort them descendingly\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6fb9599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['expnum', 'attr7_2', 'sinc7_2', 'intel7_2', 'fun7_2', 'amb7_2',\n",
       "       'shar7_2', 'numdat_3', 'num_in_3', 'attr7_3', 'sinc7_3', 'intel7_3',\n",
       "       'fun7_3', 'amb7_3', 'shar7_3', 'shar2_3', 'attr5_3', 'sinc5_3',\n",
       "       'intel5_3', 'fun5_3', 'amb5_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retreive the columns that has null values > 65% and since more than .65 has null values \n",
    "null_cols = df.columns[df.isnull().sum()/df.shape[0] >.65]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a27e6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=null_cols, inplace = True)\n",
    "df1.drop(columns=null_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51d17f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5909, 171), (2469, 170))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape,df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d442813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shar4_3     3838\n",
       "fun2_3      3838\n",
       "attr4_3     3838\n",
       "sinc4_3     3838\n",
       "intel4_3    3838\n",
       "            ... \n",
       "position       0\n",
       "round          0\n",
       "wave           0\n",
       "condtn         0\n",
       "id             0\n",
       "Length: 171, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "680250ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the input and the output\n",
    "X = df.drop('match',axis = 1)\n",
    "y = df['match']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cde1e",
   "metadata": {},
   "source": [
    "## splitting the data into 80% training and 20% Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01cb5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50601359",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col=X_train.select_dtypes(include=['object'])\n",
    "for col in object_col.columns.values:\n",
    "    df[col] = X_train[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d743b37",
   "metadata": {},
   "source": [
    "## A Tunable Pipeline\n",
    "\n",
    "In fact, any preprocessing steps can be considered as part of model and different configurations for these steps can be tunable. We can combine preprocessing steps and model as a single tunable pipeline with hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "33620452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric features: ['gender', 'idg', 'condtn', 'wave', 'round', 'position', 'positin1', 'order', 'partner', 'pid', 'int_corr', 'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o', 'age', 'field_cd', 'race', 'imprace', 'imprelig', 'goal', 'date', 'go_out', 'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob', 'met', 'match_es', 'attr1_s', 'sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s', 'sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s', 'satis_2', 'length', 'numdat_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', 'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2', 'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', 'intel5_2', 'fun5_2', 'amb5_2', 'you_call', 'them_cal', 'date_3', 'attr1_3', 'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr4_3', 'sinc4_3', 'intel4_3', 'fun4_3', 'amb4_3', 'shar4_3', 'attr2_3', 'sinc2_3', 'intel2_3', 'fun2_3', 'amb2_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3', 'id']\n",
      "categorical features: []\n"
     ]
    }
   ],
   "source": [
    "# we extract numeric features and categorical features names\n",
    "# for later use\n",
    "\n",
    "# numeric features can be selected by: (based on the df2.info() output )\n",
    "features_numeric = list(X_train.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# categorical features can be selected by: (based on the df2.info() output )\n",
    "features_categorical = list(X_train.select_dtypes(include=['category']))\n",
    "\n",
    "print('numeric features:', features_numeric)\n",
    "print('categorical features:', features_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58ea67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0) #every time you call the numpy's other random function, the result will be the same\n",
    "\n",
    "# define a pipe line for numeric feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer()),#Imputation transformer for completing missing values.\n",
    "        ('scaler', StandardScaler())]#Standardize features\n",
    "\n",
    ")\n",
    "\n",
    "# define a pipe line for categorical feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant')), #here I set the strategy for completting missing values  = constant\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')) #Encode categorical features as a one-hot numeric\n",
    "        #When handle_unknown parameter is set to ‘ignore’ and an unknown category is encountered during transform,\n",
    "        #the resulting one-hot encoded columns for this feature will be all zeros.\n",
    "\n",
    "    ]\n",
    ")\n",
    "# define the preprocessor \n",
    "# we gave them a name so we can set their hyperparameters\n",
    "# we also specify what are the categorical \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cae76a",
   "metadata": {},
   "source": [
    "## Grid Search with Cross-validation\n",
    "\n",
    "We can tune the hyperparameters (including different preprocessing configurations using cross-validation and grid-search). Grid search tests all possible combination of hyperparameters by the options and ranges you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f60e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['gender', 'idg', 'condtn',\n",
       "                                                   'wave', 'round', 'position',\n",
       "                                                   'positin1', 'order',\n",
       "                                                   'partner', 'pid', 'int_corr',\n",
       "                                                   'samerace', 'age_o',\n",
       "                                                   'race_o', 'pf_o_att',\n",
       "                                                   'pf_o_sin', 'pf_o_int',\n",
       "                                                   'pf_o_fun', 'pf_o_amb',\n",
       "                                                   'pf_o_sha', 'attr_o',\n",
       "                                                   'sinc_o', 'intel_o', 'fun_o',\n",
       "                                                   'amb_o', 'shar_o', 'like_o',\n",
       "                                                   'prob_o', 'met_o', 'age', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  [])])),\n",
       "                ('my_classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the preprocessor with the model as a full tunable pipeline\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "RF_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           RandomForestClassifier(), #I want to use random forest classifier\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "RF_pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cc14816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pipeline object can be used like any sk-learn model\n",
    "RF_pipline = full_pipline.fit(X_train, y_train)\n",
    "RF_pipline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4df3ea",
   "metadata": {},
   "source": [
    "## 1st Trial Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29381556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "best score 0.8535789105295903\n",
      "best parameters {'my_classifier__max_depth': 170, 'my_classifier__n_estimators': 180, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with Cross-validation\n",
    "# here we specify the search space\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean'], #I set it to mean\n",
    "    'my_classifier__n_estimators': [50, 100, 150],  # ranges of n_estimators which are the number of trees to be used in the forest.\n",
    "    # I set my  n_estimators ranges to [50, 100, 150]\n",
    "    'my_classifier__max_depth':[30, 60, 90]  \n",
    "    # I set my  max depth ranges to [30, 60, 90] which are The number of splits that each decision tree is allowed to make.\n",
    "}\n",
    "\n",
    "# five-fold cross-validation\n",
    "# n_jobs means cucurrent number of jobs is 2\n",
    "RF_GSearch = GridSearchCV(\n",
    "    RF_pipline, param_grid, cv=5, verbose=1 # showing more 'wordy' information\n",
    "    , n_jobs=2, \n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "RF_GSearch.fit(X, y) #fitting X and y\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_)) #printing best score\n",
    "print('best parameters {}'.format(grid_search.best_params_))#printing best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8772d162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_GSearch.predict(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d19cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = grid_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('sample_submission_walkthrough.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b51ab",
   "metadata": {},
   "source": [
    "## Trial one using RandomForest with Grid Search with Cross-validation\n",
    "Cross-validation is a method for robustly estimating test-set performance (generalization) of a model. Grid-searching is the process of scanning the data to configure optimal parameters for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01865",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "I expect to get the optimal hyperparamters that will give me the best performance and highest accuracy using random forest model with grid search and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02e8fb",
   "metadata": {},
   "source": [
    "## Observation\n",
    "After running the code,the best hyperparameters that was defined by the grid search using cross validation were :\n",
    "\n",
    "max_depth: 90\n",
    "\n",
    "n_estimators: 150\n",
    "\n",
    "with using imputer__strategy of: 'mean'\n",
    "\n",
    "The model gave me a best score of 0.8597120781837011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ea75d",
   "metadata": {},
   "source": [
    "## Plan\n",
    "I'm going to set different ranges for the hyperparameters in the grid search with cross validation\n",
    "\n",
    "n_estimators': [90, 180, 270 ]\n",
    "\n",
    "max_depth':[30, 100, 170]\n",
    "\n",
    "and I changed my imputer__strategy' to 'most_frequent'\n",
    "\n",
    "and I changed number of folds to cv=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f9ea9",
   "metadata": {},
   "source": [
    "## 2nd Trial Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec88cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "best score 0.8535789105295903\n",
      "best parameters {'my_classifier__max_depth': 170, 'my_classifier__n_estimators': 180, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with Cross-validation\n",
    "# here we specify the search space\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['most_frequent'], #I set it to most frequent\n",
    "    'my_classifier__n_estimators': [90, 180, 270],  # ranges of n_estimators which are the number of trees to be used in the forest.\n",
    "    # I set my  n_estimators ranges to [90, 180, 270]\n",
    "    'my_classifier__max_depth':[30, 100, 170]  \n",
    "    # I set my  max depth ranges to [30, 100, 170] which are The number of splits that each decision tree is allowed to make.\n",
    "}\n",
    "\n",
    "# four-fold cross-validation\n",
    "# n_jobs means cucurrent number of jobs is -1\n",
    "RF_GSearch = GridSearchCV(\n",
    "    RF_pipline, param_grid, cv=10, verbose=1 # showing more 'wordy' information\n",
    "    , n_jobs=-1, \n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "RF_GSearch.fit(X_train, y_train) #fitting X and y\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_)) #printing best score\n",
    "print('best parameters {}'.format(grid_search.best_params_))#printing best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "541b3a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_GSearch.predict(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c48234",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = grid_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('RF_GS.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236ca38",
   "metadata": {},
   "source": [
    "## 2nd Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e97894",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "Expected higher score after changing the hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b806",
   "metadata": {},
   "source": [
    "## Observation\n",
    "After running the code,the best hyperparameters that was defined by the grid search using cross validation were :\n",
    "\n",
    "max_depth: 30\n",
    "\n",
    "n_estimators: 2700\n",
    "\n",
    "with using imputer__strategy of: 'most_frequent'\n",
    "\n",
    "The model gave me a best score of 0.861641763794065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39feb89a",
   "metadata": {},
   "source": [
    "## 1st Trial Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ee75651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['gender', 'idg', 'condtn',\n",
       "                                                   'wave', 'round', 'position',\n",
       "                                                   'positin1', 'order',\n",
       "                                                   'partner', 'pid', 'int_corr',\n",
       "                                                   'samerace', 'age_o',\n",
       "                                                   'race_o', 'pf_o_att',\n",
       "                                                   'pf_o_sin', 'pf_o_int',\n",
       "                                                   'pf_o_fun', 'pf_o_amb',\n",
       "                                                   'pf_o_sha', 'attr_o',\n",
       "                                                   'sinc_o', 'intel_o', 'fun_o',\n",
       "                                                   'amb_o', 'shar_o', 'like_o',\n",
       "                                                   'prob_o', 'met_o', 'age', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  [])])),\n",
       "                ('my_classifier', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           DecisionTreeClassifier(), #I want to use random forest classifier\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "DT_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0677dcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_pipeline = DT_pipeline.fit(X_train,y_train)\n",
    "DT_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2bcec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n",
      "best score 0.8154633972147636\n",
      "best score {'my_classifierdes__criterion': 'entropy', 'my_classifierdes__max_depth': 4, 'my_classifierdes__min_samples_leaf': 30}\n"
     ]
    }
   ],
   "source": [
    "decision_search={\n",
    "    \"my_classifierdes__criterion\" : [\"gini\" , \"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [2 , 4 , 6 , 8, 10, 12],\n",
    "     \"my_classifierdes__min_samples_leaf\": [10,11,20,30]\n",
    " }\n",
    "decision_search = GridSearchCV(\n",
    "    DT_pipeline, decision_search, cv=10, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8adf4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659b35b",
   "metadata": {},
   "source": [
    "## Decision Tree 2nd Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b265fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 80 candidates, totalling 1200 fits\n",
      "best score 0.8140942286621417\n",
      "best score {'my_classifierdes__criterion': 'entropy', 'my_classifierdes__max_depth': 5, 'my_classifierdes__min_samples_leaf': 30}\n"
     ]
    }
   ],
   "source": [
    "decision_search={\n",
    "    \"my_classifierdes__criterion\" : [\"gini\" , \"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [2, 4, 6, 5, 10, 15, 20, 25],\n",
    "     \"my_classifierdes__min_samples_leaf\": [5,10,15,20,30],\n",
    " }\n",
    "decision_search = GridSearchCV(\n",
    "    decision_pipline, decision_search, cv=15, verbose=1, n_jobs=5,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec927aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = decision_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('DT_GS.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fc3b6",
   "metadata": {},
   "source": [
    "## Bayesian Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f607b7",
   "metadata": {},
   "source": [
    "## XGB 1st Trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bae83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "best score 0.8773251599351495\n",
      "best score OrderedDict([('my_classifier__learning_rate', 0.05), ('my_classifier__max_depth', 7), ('my_classifier__n_estimators', 500)])\n"
     ]
    }
   ],
   "source": [
    "XGBpipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           XGBClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "XGBbayes_search = BayesSearchCV(\n",
    "    XGBpipline,\n",
    "    {\n",
    "    'my_classifier__max_depth':[4,5,7,],\n",
    "    'my_classifier__n_estimators': [20, 50,100, 400,500,600],\n",
    "    'my_classifier__learning_rate': [0.1, 0.5, 0.05]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=10,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=10, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "XGBbayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d412613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBbayes_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b586fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = bayes_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('XGB_BS.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fbc1d",
   "metadata": {},
   "source": [
    "## SVM 1st Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42afc326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "best score 0.8254702059353223\n",
      "best parameters OrderedDict([('my_svc__C', 0.00013612772657396573), ('my_svc__degree', 8), ('my_svc__gamma', 1.214461458157585), ('my_svc__kernel', 'poly')])\n"
     ]
    }
   ],
   "source": [
    "SVC_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_svc', SVC(class_weight='balanced',probability=True)) #setting probability to true to be able to use probna() later when submission\n",
    "        #class_weight='balanced' assigns the class weights inversely proportional to their respective frequencies\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define ranges for bayes search\n",
    "bayes_search = BayesSearchCV(\n",
    "    SVC_pipline,\n",
    "    {\n",
    "        'my_svc__C': Real(1e-8, 1e+8, prior='log-uniform'), #ranges for c which is the Penalty parameter of the error term.\n",
    "        'my_svc__gamma': Real(1e-9, 1e+1, prior='log-uniform'), #ranges for gamma, the gamma parameter defines how far the influence of a single training example reaches\n",
    "        'my_svc__degree': Integer(1,8), #degree ranges,the degree of the polynomial kernel function \n",
    "        'my_svc__kernel': Categorical(['poly','linear', 'rbf']), #kernel ranges,A kernel is a function used in SVM for helping to solve problems\n",
    "     \n",
    "    },\n",
    "    # number of trials \n",
    "    n_iter=6,\n",
    "    random_state=0,#the random_state parameter is used for initializing the internal random number generator\n",
    "    verbose=1,# showing more 'wordy' information\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best parameters {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41d7efd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d2237262",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = bayes_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('SVC_BS.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fce32",
   "metadata": {},
   "source": [
    "## Random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9460d7",
   "metadata": {},
   "source": [
    "## 1st Trial Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66c7fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "best score 0.8134462813909783\n",
      "best score {'my_classifierdes__min_samples_leaf': 10, 'my_classifierdes__max_depth': 4, 'my_classifierdes__criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "decision_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifierdes', \n",
    "        DecisionTreeClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "decision_search={\n",
    "    \"my_classifierdes__criterion\" : [\"gini\" , \"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [2 , 4 , 6 , 8, 10, 12],\n",
    "     \"my_classifierdes__min_samples_leaf\": [10,12,15,20,25,30]\n",
    " }\n",
    "decision_search = RandomizedSearchCV(\n",
    "    decision_pipline, decision_search, cv=10, verbose=1, n_jobs=2, n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfae4a",
   "metadata": {},
   "source": [
    "## 1st Trial Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c136f224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "best score 0.8158823078345669\n",
      "best score {'my_classifierdes__min_samples_leaf': 25, 'my_classifierdes__max_depth': 4, 'my_classifierdes__criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "decision_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifierdes', \n",
    "        DecisionTreeClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "decision_search={\n",
    "    \"my_classifierdes__criterion\" : [\"gini\" , \"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [2 , 4 , 6 , 8, 10, 12],\n",
    "     \"my_classifierdes__min_samples_leaf\": [10,12,15,22,25,30]\n",
    " }\n",
    "decision_search = RandomizedSearchCV(\n",
    "    decision_pipline, decision_search, cv=10, verbose=1, n_jobs=1, n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c8115",
   "metadata": {},
   "source": [
    "## XGB 2nd Trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f5738e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "best score 0.8750893640716185\n",
      "best score {'preprocessor__num__imputer__strategy': 'most_frequent', 'my_classifier__n_estimators': 180, 'my_classifier__max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    XGBpipline, param_grid, cv=5, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8020635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "submission['id'] = df1['id'] #using the column index of x test values to fill submission['id'] column\n",
    "submission['match'] = random_search.predict_proba(df1)[:,1] #predicting the probabilities and filling submission['match'] column with their values\n",
    "submission.to_csv('XGB_RS.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ab5c3",
   "metadata": {},
   "source": [
    "## Our input\n",
    "A data set containing lots of information about each participent in a speed dating event,such as his unique subject number,wether he was a male or female, information about each wave such as the number of people he/she met in a wave,station number where met partner,partner's id, wether the participant and the partner were the same race, information about the partner him/her self such as their race,their age,their decision and their rating from 1 to 6...etc.\n",
    "\n",
    "The dataset also contained information collected from a Survey filled out by students that are interested in participating in order to register for the event Like age,field pf study and each field has its own code in our data set,their Median SAT score for their undergraduate institution and also some questions like How important was it to him/her (on a scale of 1-10) that a person they date be of the same racial/ethnic background or e be of the same religious background, and where were they originally from amd the zip code for the place they grew up in, their goal of paticipating in the event , their career, how happy does he/she expect to be with the people they meet during the speed-dating event .. etc.\n",
    "\n",
    "It also had informatiom out of a survey Filled out by subjects after each date during the event, the survey required information like their id,their decision and answers for questions about each person they meet like their attributes,how much they like the person..etc.\n",
    "\n",
    "It has also answers from each participent on Half way through meeting asking about what We want to know what each participent look for in the opposite sex.\n",
    "\n",
    "Also it has information collected from a Survey filled out the day after participating in the event like how satisfied were they with the people they met,What do they think the opposite sex looks for in a date,How does each participent think he/she measures up...etc.\n",
    "\n",
    "Last but not least,Subjects filled out 3-4 weeks after they had been sent their matches and the data set included information regarding that such as How many of the matches each paticipent had has he/she contacted to set up a date, Have the participent been on a date with any of his/her matches and how many....etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622658b",
   "metadata": {},
   "source": [
    "## Our output\n",
    "Our model is required to predict the outcome of a specific speed dating session based on the profile of two people in order to implement a recommendation system to better match people in speed dating events. we are going to predict the probability that the dating session will lead to a successful match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659089c4",
   "metadata": {},
   "source": [
    "## What data mining function is required?\n",
    "binary classification using pipelines,grid search,Random Search and Bayesian Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7af92",
   "metadata": {},
   "source": [
    "## What could be the challenges?\n",
    "Developing a successful solution to our problem , complex data,datasets can include complex data elements ,another thing is that we have to make sure that our algorithm must be efficient and scalable to extract information from the big data and we should have enough knowledge and experience in order to use them if we needed to improve our algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a62cb",
   "metadata": {},
   "source": [
    "## What is the impact?\n",
    "Our model predictions is going to implement a recommendation system to better match people in speed dating events which will make it easier when it comes to dating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e0114",
   "metadata": {},
   "source": [
    "## What is an ideal solution?\n",
    "An ideal solution in my opinion will be measured in terms of metrics and performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49916660",
   "metadata": {},
   "source": [
    "## What is the experimental protocol used and how was it carried out? What preprocessing steps are used?\n",
    "when it comes to the preprocessing steps I made a pipeline performing the following on the training set and testing set :\n",
    "\n",
    "SimpleImputer(),Imputation transformer for completing missing values.\n",
    "\n",
    "one hot encoder to transform categorical data to numerical data\n",
    "\n",
    "stadnard scalar to standard scale my data\n",
    "\n",
    "The experimental protocol I used in my code is cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80761e63",
   "metadata": {},
   "source": [
    "## Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?\n",
    "A simple linear regression model without any activation function is not suitable for classification tasks because it does not provide a binary output, which is necessary for classification. In simple linear regression, the output is a continuous value that can take any value, including negative values and values greater than 1. This makes it difficult to interpret the output as a class label.\n",
    "\n",
    "On the other hand, Perceptron/Logistic regression models are specifically designed for classification tasks. They use activation functions, such as the sigmoid function, to squash the output of the linear regression model between 0 and 1, which can be interpreted as a probability of belonging to a particular class. This makes it easier to convert the output of the model into a binary classification label.\n",
    "\n",
    "Additionally, Perceptron/Logistic regression models are more robust to outliers and noisy data compared to simple linear regression models. This is because they use a threshold function to determine the class label, which is less affected by the values of individual data points.\n",
    "\n",
    "Overall, Perceptron/Logistic regression models are better suited for classification tasks compared to simple linear regression models because they produce binary outputs, are more interpretable, and are more robust to outliers and noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17116f45",
   "metadata": {},
   "source": [
    "## What's a decision tree and how it is different to a logistic regression model?\n",
    "A decision tree is a type of machine learning algorithm used for both classification and regression tasks. It involves creating a tree-like model of decisions and their possible consequences. The tree is constructed by recursively splitting the data into subsets based on the values of the input features, until each subset contains only one class or a maximum depth is reached. The final result is a series of if-then statements that can be used to make predictions on new data.\n",
    "\n",
    "Logistic regression, on the other hand, is a statistical model used for binary classification tasks. It involves modeling the probability of an event occurring based on one or more input variables. The goal of logistic regression is to find the coefficients of the input variables that maximize the likelihood of the observed data.\n",
    "\n",
    "The main difference between decision trees and logistic regression models is in their approach to modeling the relationship between the input variables and the output variable. Decision trees recursively partition the input space into regions that correspond to specific class labels, whereas logistic regression models estimate the probability of belonging to a specific class based on the values of the input variables.\n",
    "\n",
    "Decision trees are generally more interpretable than logistic regression models, as they provide a clear and intuitive representation of the decision-making process. They can also handle non-linear relationships between the input variables and the output variable, and are more robust to noise and outliers. However, they can be prone to overfitting, especially when the tree is deep or the data is high-dimensional.\n",
    "\n",
    "Logistic regression models, on the other hand, are generally more efficient and have better statistical properties, such as convergence guarantees and asymptotic normality of the parameter estimates. They can also handle continuous and categorical input variables, and are less prone to overfitting than decision trees. However, they can be less interpretable than decision trees, especially when the model has many input variables or complex interactions between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5881d2",
   "metadata": {},
   "source": [
    "## What's the difference between grid search and random search?\n",
    "Grid search and random search are two popular hyperparameter tuning techniques used in machine learning.\n",
    "\n",
    "Grid search involves exhaustively searching over a pre-defined set of hyperparameters by evaluating the model performance on a validation set for each combination of hyperparameters. Grid search is systematic and guarantees that all possible combinations of hyperparameters are evaluated, but it can be computationally expensive and time-consuming, especially for a large number of hyperparameters or large ranges of hyperparameter values.\n",
    "\n",
    "Random search, on the other hand, involves randomly sampling from a pre-defined distribution of hyperparameters and evaluating the model performance on a validation set for each set of hyperparameters. Random search is less systematic than grid search, but can be more efficient in exploring the hyperparameter space, especially when the number of hyperparameters is large or the optimal hyperparameters are not known in advance. Random search can also help avoid local minima and discover unexpected combinations of hyperparameters.\n",
    "\n",
    "Overall, grid search is a good choice when the hyperparameter space is relatively small and the optimal hyperparameters are expected to be near the center of the hyperparameter space, while random search is a good choice when the hyperparameter space is large or the optimal hyperparameters are not known in advance. In practice, a combination of both techniques can be used to achieve a good balance between systematic exploration and efficient search of the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798eb29f",
   "metadata": {},
   "source": [
    "## What's the difference between bayesian search and random search?\n",
    "\n",
    "Bayesian search and random search are two popular hyperparameter tuning techniques used in machine learning.\n",
    "\n",
    "Random search involves randomly sampling hyperparameters from a pre-defined distribution and evaluating the model performance on a validation set for each set of hyperparameters. Random search is flexible and easy to implement, but can be inefficient in high-dimensional search spaces and often requires a large number of samples to find optimal hyperparameters.\n",
    "\n",
    "Bayesian search, on the other hand, uses a probabilistic model to predict the performance of different hyperparameter configurations and selects the next set of hyperparameters to evaluate based on the expected improvement over the current best configuration. Bayesian search is more efficient than random search in high-dimensional search spaces and can often find optimal hyperparameters with fewer samples.\n",
    "\n",
    "The main difference between Bayesian search and random search is that Bayesian search uses a probabilistic model to guide the search process, while random search selects hyperparameters randomly. Bayesian search also takes into account the uncertainty in the model predictions and adjusts the search process accordingly, whereas random search does not consider the performance of previously evaluated hyperparameter configurations.\n",
    "\n",
    "Overall, Bayesian search is a good choice when the hyperparameter space is high-dimensional and the optimal hyperparameters are not known in advance, while random search is a good choice when the hyperparameter space is small or the optimal hyperparameters are expected to be near the center of the search space. In practice, a combination of both techniques can be used to achieve a good balance between systematic exploration and efficient search of the hyperparameter space."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
